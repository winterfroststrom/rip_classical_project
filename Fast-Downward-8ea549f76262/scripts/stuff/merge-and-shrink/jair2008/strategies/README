-------------------------------
run-strategy-BLA:

all-problems: should be clear 

icaps-problems: icaps subset of each domain

S11R1GB3-problems: subset solved by strategy 11 with 1 abstraction in
3GB mem limit. Results were obtained by determining this subset first,
then running all other strategies on that subset. NOTE: it may
theoretically be that one of the other strategies solves more
problems; seems unlikely, though.


-------------------------------

Usage run-strategy-BLA:

run-strategy-BLA xy n m

where "xy" is strategy ie arg "Sxy", n is start rnd index, m is end
rnd index (enables separate runs for same config).

To avoid confusion in initialization, removed the making/removing of
results dirs. 

NOTE: needs to be run in a subdir of scripts dir!!

Other scripts arranged to allow flexible overall run:


-------------------------------
initialize-runs

cleans up the results dir and creates the required subdirectories


-------------------------------
summarize-runs

creates all the summary files
(EXTEND later to also do the mean values)


-------------------------------
cleanup-summaries

creates uniform format files with data for unsolved problems -1 total
runtime; takes median out of each set of runs for each problem, for
the randomized strategies



-------------------------------

After the cleaned up summaries are there, one can run


make-overview-runtime/extract


this MUST be run in its subdir, since it looks up the CLEAN- files
according to a relative directory path. it takes as arguments a subset
of the flags {-1, -2, -3, -4, -5, -6, -7} corresponding to the subset
of strategies to be considered (see usage info when run with some
random argument). it prints out sumamrized #solved and mean runtime
results. the subset of strategies is important because means are taken
over all instances solved by ALL selected strategies. the results are
organized on a per-domain basis, and both absolute data and relative
data (to main, which is always included) are shown. NOTE: the domin
info is not present in the cleaned up files. I reconstruct it in here
via observing changes in the N values (ie the "a" parameter of
search). I know this is an awful HACK, but whatever. when adding a new
domain, one simply has to insert the name of the domain into a string
array at the start of the program; the position in the array must be
the position of the domain as it appears in the SUMMARY files (ie,
alphabetic). NOTE that this only works if N is kept constant on a
per-domain basis.


make-overview-htime/extract: similar for h fn construct time
make-overview-S/extract: similar for nr search states total
make-overview-Slb/extract: similar for nr search states up to highest lower bound


see summary tables & discussion in paper.





-------------------------------




giving the SUMMARY file as arguments (same call args as in the script
cleanup-summaries), one can use

cleanup-summaries/extract

to get infos on average std deviation per domain (always) and
individual std deviation per task (if switched on in code). deviation
is computed ONLY FOR THE PROBLEMS WHERE ALL RUNS WERE SUCCESSFUL.


however, the observations don't seem interesting. basically, deviation
is quite high for all the random methods, in particular for 21 and 41,
since in all the domains the variance in size of search space is quite
high, ie the quality of the h fn differs quite a bit between runs
(well the extent of this phenomenon depends on the domain; very high
in log and psr, not quite so high in the rest). for 14, the std dev is
much smaller than for 21 and 41, but that's only because 14 is so bad
and solves only very small examples, where the variance does not yet
occur either for 41 and 21. an interesting bit here is that 14 almost
always fails DURING H FN CONSTRUCTION. Malte, you should insert some
explanation in the paper as to what the reason for that is.



